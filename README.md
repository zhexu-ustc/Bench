# Bench
This benchmark focuses on the interpretability of Large Vision-Language Models (LVLMs) and is designed to detect spurious biases in object recognition within LVLMs. It consists of two datasets:
1. Original images and their corresponding Visual Question Answering (VQA)
2. Edited versions of the original images with counterfactual objects introduced, along with their corresponding VQA

! [示例]case/case.png
